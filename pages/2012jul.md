# July 2012 Log

## 08 July
A new attempt at a blog format. This totally isn't a thing that I do all the time.

Since I last made attempts to keep a log of my doings, I've acquired ideas for research topics that are more specific than any other I've had since I started this fool's errand nearly three years ago. It started with realizing that embodiment was a pretty keen idea, but it is, of course, not enough to chant "embodied cognition" and expect to solve the Synthetic Minds Problem (roughly stated: how can we make one?). Embodiment was an idea I got turned onto in May, but it wasn't until a couple weeks ago that I became taken with understanding sensorimotor dynamics. I'm convinced that sensorimotor business is hugely important for perception (the sensor that moves itself!), especially spatial perception. The work I'm looking at now is Lungarella and Sporns 2006 and some earlier work by Lungarella et al on "information self-structuring" which claims to show that considering motor interaction along with sensory input has information theoretic benefits. I haven't looked in detail since I'm still brushing up on my information theory, but I'm alarmed in that I didn't see much followup work to this (as determined by clicking "Cited by" in google scholar and skimming article titles/abstracts). It might be an insight but it's clearly not enough, so I need to find another direction to move in after this.

One thing I've been somewhat fixated on is understanding proprioception and especially the vestibular system. My (mostly uninformed) hunch is that both are essential to spatial perception in vertebrates. I'm torn on how much biomechanics is needed, despite my affinity for the "embodiment" insight. It's probably totally critical for understanding sensorimotor neuroscience (the motor signals are commands to a biomechanical machine, so it makes sense that you need to understand the biomechanics). However, it's not clear that modelling a realistic musculoskeletal system would provide insights to spatial perception that modelling some crappy artificial robot limbs would not. Maybe animal-like spatial perception needs the intricately controlled and flexible limbs that evolution has gifted us. Hrnm.

Current focus is Elements of Information Theory by Cover and a paper on "Transfer Entropy", since the latter is a major measure used in the 2006 paper. I'm also making a notes page on information theory to help me retain this better (I've definitely read chapter 2 of Cover before and then forgot most of it, but IIRC that was during my post-bachelors SIUC days when I wasn't so great at learning).

## 12 July
Today I read ["A Sense of Crisis: Physics in the fin-de-siecle Era"](http://arxiv.org/abs/1207.2016), which described the various dead-end theories entertained by physicists and fringe intellectuals around the turn of the 19th century. It was quite a contrast to the accounts I've read of the development of quantum mechanics, which obviously were not written by philosophers of history. There were many fantastical and often idealistic ideas that turned out to be just wrong. Some of it was clearly deluded and not scientifically motivated, but it seemed like the attempt to do away with mechanics and explain everything in terms of electromagnetism was a serious attempt by real physicists which just didn't work. It is interesting to think about which branches of investigation today will be nonexistent in 10-20 years. What is the 21st-century equivalent of energetics?

More work done on my information theory notes, though it goes slowly. Entropy section is solid, but that's about it. I had to spend some time mucking about with formatting issues: getting MathJax to display what I want, getting an image sized appropriately, various CSS pretties. I need to improve my static site generator but I do not want to spend the time. I'd much rather be learning.

Speaking of which, in the past few days I've gone over most of Chapter 2 of Cover and Joy. I need to hit it again, but I'm skipping ahead to Chapter 4 real quick to butt heads with entropy rate. It has a brief overview of stochastic processes, which is nice as I never covered that in depth and am not quite as strong on it as I'd like to be.

I believe I've discovered that "chunking" math definitions helps me remember them/pseudo-understand them so much better. Example: a stationary stochastic process. First define a "window" to be some finite sequence of consecutive random variables in the stochastic process. A *stationary* process is then one such that for any fixed length, every window has the same joint distribution. The me of two years ago would've just tried to memorize the symbols: for any n and any k, Pr(X1 = x1, ..., Xn = xn) = Pr(X{k+1} = x{k+1}, ..., X{k+n} = x{k+n}). Nonsense. Put it in words, even if you have to make up some words. (Better yet, do both. The words and the symbols reinforce).

## 13 July
On my morning commute I realized why time homogeneity does not imply stationarity for a Markov chain. Consider only windows of length two. P(X1, X2) = P(X2 | X1) P(X1). P(X2, X3) = P(X3 | X2) P(X2). By time homogeneity, P(X3 | X2) = P(X2 | X1), so P(X2, X3) = P(X2 | X1) P(X2). To get stationarity it is necessary to show, then, that P(X1) = P(X2). So in addition to time homogeneity, the variables have to be identically distributed. This is necessary, and I'm pretty sure sufficient as well. For any n-window P(Xk, Xk+1, ..., Xk+n-1) = P(X2 | X1) P(Xk) by time homogeneity and the Markov property. But P(Xk) = P(X1) due to identically distributed r.v.'s. Blah blah blah.

I spent a lot of time thinking about self information and info-theoretic analysis of Bayes theorem, prompted by the reading of a LW post that I'm too lazy to link. The article had some very good ideas but also contained significan problems, and I am considering doing a "rewrite" to fix them. The first would be fixing a core definition critical to the point of the paper (information evidence), which the author managed to incorrectly negate. Also the paper uses two different kinds of bits which are unrelated: first as an encoding for describing outcomes and deriving their probabilities, and second as a unit of self-information. I would describe the outcomes as flips of a fair coin and get rid of the first bit usage. The paper also needs another example to illustrate both cases of "informational evidence": positive info ev and negative info ev. Also, I'm not  quite clear on whether it makes sense to think of self-information as involving "uncertainty". Hence, the author's core point, that "Bayesian updating = subtracting mutual evidence from uncertainty", may not be quite correct, though it certainly sounds compelling. Also, I'm not sure on the interpretation of "negative mutual evidence".
