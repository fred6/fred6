# July 2012 Log

## 08 July
A new attempt at a blog format. This totally isn't a thing that I do all the time.

Since I last made attempts to keep a log of my doings, I've acquired ideas for research topics that are more specific than any other I've had since I started this fool's errand nearly three years ago. It started with realizing that embodiment was a pretty keen idea, but it is, of course, not enough to chant "embodied cognition" and expect to solve the Synthetic Minds Problem (roughly stated: how can we make one?). Embodiment was an idea I got turned onto in May, but it wasn't until a couple weeks ago that I became taken with understanding sensorimotor dynamics. I'm convinced that sensorimotor business is hugely important for perception (the sensor that moves itself!), especially spatial perception. The work I'm looking at now is Lungarella and Sporns 2006 and some earlier work by Lungarella et al on "information self-structuring" which claims to show that considering motor interaction along with sensory input has information theoretic benefits. I haven't looked in detail since I'm still brushing up on my information theory, but I'm alarmed in that I didn't see much followup work to this (as determined by clicking "Cited by" in google scholar and skimming article titles/abstracts). It might be an insight but it's clearly not enough, so I need to find another direction to move in after this.

One thing I've been somewhat fixated on is understanding proprioception and especially the vestibular system. My (mostly uninformed) hunch is that both are essential to spatial perception in vertebrates. I'm torn on how much biomechanics is needed, despite my affinity for the "embodiment" insight. It's probably totally critical for understanding sensorimotor neuroscience (the motor signals are commands to a biomechanical machine, so it makes sense that you need to understand the biomechanics). However, it's not clear that modelling a realistic musculoskeletal system would provide insights to spatial perception that modelling some crappy artificial robot limbs would not. Maybe animal-like spatial perception needs the intricately controlled and flexible limbs that evolution has gifted us. Hrnm.

Current focus is Elements of Information Theory by Cover and a paper on "Transfer Entropy", since the latter is a major measure used in the 2006 paper. I'm also making a notes page on information theory to help me retain this better (I've definitely read chapter 2 of Cover before and then forgot most of it, but IIRC that was during my post-bachelors SIUC days when I wasn't so great at learning).

## 12 July
Today I read ["A Sense of Crisis: Physics in the fin-de-siecle Era"](http://arxiv.org/abs/1207.2016), which described the various dead-end theories entertained by physicists and fringe intellectuals around the turn of the 19th century. It was quite a contrast to the accounts I've read of the development of quantum mechanics, which obviously were not written by philosophers of history. There were many fantastical and often idealistic ideas that turned out to be just wrong. Some of it was clearly deluded and not scientifically motivated, but it seemed like the attempt to do away with mechanics and explain everything in terms of electromagnetism was a serious attempt by real physicists which just didn't work. It is interesting to think about which branches of investigation today will be nonexistent in 10-20 years. What is the 21st-century equivalent of energetics?

More work done on my information theory notes, though it goes slowly. Entropy section is solid, but that's about it. I had to spend some time mucking about with formatting issues: getting MathJax to display what I want, getting an image sized appropriately, various CSS pretties. I need to improve my static site generator but I do not want to spend the time. I'd much rather be learning.

Speaking of which, in the past few days I've gone over most of Chapter 2 of Cover and Joy. I need to hit it again, but I'm skipping ahead to Chapter 4 real quick to butt heads with entropy rate. It has a brief overview of stochastic processes, which is nice as I never covered that in depth and am not quite as strong on it as I'd like to be.

I believe I've discovered that "chunking" math definitions helps me remember them/pseudo-understand them so much better. Example: a stationary stochastic process. First define a "window" to be some finite sequence of consecutive random variables in the stochastic process. A *stationary* process is then one such that for any fixed length, every window has the same joint distribution. The me of two years ago would've just tried to memorize the symbols: for any n and any k, Pr(X1 = x1, ..., Xn = xn) = Pr(X{k+1} = x{k+1}, ..., X{k+n} = x{k+n}). Nonsense. Put it in words, even if you have to make up some words. (Better yet, do both. The words and the symbols reinforce).

## 13 July
On my morning commute I realized why time homogeneity does not imply stationarity for a Markov chain. Consider only windows of length two. P(X1, X2) = P(X2 | X1) P(X1). P(X2, X3) = P(X3 | X2) P(X2). By time homogeneity, P(X3 | X2) = P(X2 | X1), so P(X2, X3) = P(X2 | X1) P(X2). To get stationarity it is necessary to show, then, that P(X1) = P(X2). So in addition to time homogeneity, the variables have to be identically distributed. This is necessary, and I'm pretty sure sufficient as well. For any n-window P(Xk, Xk+1, ..., Xk+n-1) = P(X2 | X1) P(Xk) by time homogeneity and the Markov property. But P(Xk) = P(X1) due to identically distributed r.v.'s. Blah blah blah.

I spent a lot of time thinking about self information and info-theoretic analysis of Bayes theorem, prompted by the reading of a LW post that I'm too lazy to link. The article had some very good ideas but also contained significan problems, and I am considering doing a "rewrite" to fix them. The first would be fixing a core definition critical to the point of the paper (information evidence), which the author managed to incorrectly negate. Also the paper uses two different kinds of bits which are unrelated: first as an encoding for describing outcomes and deriving their probabilities, and second as a unit of self-information. I would describe the outcomes as flips of a fair coin and get rid of the first bit usage. The paper also needs another example to illustrate both cases of "informational evidence": positive info ev and negative info ev. Also, I'm not  quite clear on whether it makes sense to think of self-information as involving "uncertainty". Hence, the author's core point, that "Bayesian updating = subtracting mutual evidence from uncertainty", may not be quite correct, though it certainly sounds compelling. Also, I'm not sure on the interpretation of "negative mutual evidence".

## 18 July
Site change: new stub of a page on my keyboard layouat qwkrfy3. For posterity. More needs to come.

I've been working a lot on a new python program that im calling [slob](http://github.com/fred6/slob). The idea is to use it as a metadata/logging system for all information sources. For instance, I download a new journal article, skim it, decide it's worth keeping, add it into the system along with an alias (short name) that I can use to refer. I can add keywords to the object. Then, as I do things involving the informatoin source like read it or think about it, I can take notes on these things. The system logs timestamps whenever I enter a note, so I will have a more-complete record of my studies. I'm operating on the principle of "what can be measured can be improved." Also I needed a better organizational system than  filesystem. Not much chance to use it yet since I've been coding, but  I think it may be usable now.

Since the last update, one of the things I encountered was the "equilibrium-point" hypothesis from Feldman. I don't understand it yet because I lack prereqs. My plan is to tackle the motor chapters in Kandel, but I'm not sure it will be sufficient. We'll see. The theory is neurophysiological in nature, so it may be a good start. Beyond that, still reading Hibbeler. I'm into chapter 4 now: static equilibrium.

I need to put into writing my newest ideas about research directions. This is a delicate procedure, because on the one hand failing to pin down your ideas allows you to handwave and avoid the pain of confronting bad ideas. On the other hand, it seems like you can try to pin down your ideas too hard and end up not handwaving enough, bewitching yourself ala Wittgenstein in the process. These damn words we have are often not good enough, often because they come saddled with all sorts of  meanings that we did not intend.

The word problem is something I've found myself being perhaps overly-concerned with. When we use words like "consciousness" and "mind" and "intelligence", these words, being members of the vernacular, often have hidden connotations that can quietly poison one's thinking. I've been tempted substitute nonsense words like "blorp" and "hubzizzle" in my thinking in order to avoid it. That may be unnecessary, but I am trying to deprecate certain terms in my thinking, believing they are confused or, due to popular use, too vague to be useful.

(Ugh, this is really badly organized. Mostly because I'm very tired. But the show must go on.)

So, what do I think. The first thought is that "AI" is dead. I've killed that term in my internal language. I only use it if I feel the need to when engaging with other people,  but "AI" and "intelligence" in general is a non-concern for me. Synthetic intelligence seems to be a massively hard problem, and it seems to me that humanity's best science and engineering and philosophy lacks the tools to productively tackle it. So put it out of your mind.

Another thing I think is that embodiment is one of the first clear insights into the synthetic mind problem. I don't want to endorse any general "embodied mind" theories, since every time I read philosophical cognitive science on this I can never make sense of it (either they're confused or I'm stupid). However, I want to endorse an extremely restricted thesis: **embodiment is essential for spatial perception/cognition/understanding/blah, and spatial blah is essential for certain types of mathematical "thinking" that humans engage in** (whatever those words mean).

I shall call this the **spatial blah problem**, where "blah" is some word referencing an understanding that we lack that means something like "perception/cognition/understanding/familiarity".

The immediate problem that we see is that the question here is ill-defined. So the problem isn't "what is spatial blah?". The problem is "define the spatial blah problem with sufficient detail that it can be solved or partially pinned down." We have to figure out what "blah" is in order to ask a question about it.

Lest this sound silly, I want to explain that I believe words have power. Using a familiar word to describe something unknown and not understand gives one a false impression of familarity. This phenomenon that I am endeavoring to study is unknown to me, and the terminology used should reflect that. I don't want to sneak in unexamined background assumptions.

Now, that's very nice and all, but using nonstandard terminology doesn't actually remove all background assumptions here. It just helps to avoid sneaking some in. I still have the fact that I was led to consider this problem only after becoming interested in AI and "minds" and such. So I do have some ideas, which I should now explicate lest I allow myself any opportunities for handwaving or sneaking in bad intuitions.

Unfortunately I don't think I've analyzed this in-depth and I'm very tired so nothing comes to mind right now. I will do the second-best thing and list some topics of study that seem promising

 - Motor control - people have been studying motor control for a long time, surely they've got some insights
 - Developmental perspective - Baby animals are wobbly and generally useless. Taking the developmental perspective may shed light on biological spatial blah.
 - Synthetic methodolgy - biological spatial blah is the inspiration, but building/simulation things can be a good source of intuition. also building things can allow you to test theories that are not feasible with living things. also "what i cannot create, i do not understand" and all that. (mind you, creatability is necessary, not sufficient).

## 23 July
Few new things. I upgraded to netcfg rather than relying on a hacked together shell script that launches wpa_supplicant and dhcpd. omg wifi-menu is nice

I made what is, I think, a substantial improvement to my static site generator. The python markdown parsers suck, so instead of using something that only slightly sucks (misaka, which wouldnt be bad except for the lack of fucking markdown footnote support in sundown. Are you serious? No footnote support? You can't be serious...), I decided to fight down  my aversion to PYTHON IMPURITY and just use pandoc instead. Nothing fancy, just a call via subprocess. I got footnotes working but I still need to look into Mathjax output. It seems to not be exactly what I was getting from misaka (seems to be parsing it, which is nice.)

I was inspired to get my website shit together more after reading [gwern's](http://gwern.net) About page. The scarcest fragments can become coherent pieces over time. Once you start thinking on the timescale of decades, you realize that the software you use needs to be something that has a good chance of being around for awhile. Pandoc, Python 3, Jinja2 and PyYAML is in my arsenal now. Pandoc (and Haskell) seems strong. Python isn't going anywhere for a bit. PyYAML is the reference implementation for YAML, so will be around as long as YAML is. Jinja2 is popular at the moment but who knows where it will be in 10 years.

I started reading a new book over the weekend after coming to the conclusion that the one Hibbeler book I was reading had too brief an account of rigid equilibrium for me to make much sense of it (I wasn't able to solve really basic problems. Uh oh). So I got a Hibbeler book that covers *only* statics, not a combination statics/mechanics book. I *thoroughly* covered equilibrium of particles, and am skimming the moments chapter (not doing exercises since I covered that pretty thoroughly in the other book). I hope to start this new rigid equilibrium chapter soon.


