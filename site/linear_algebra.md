# linear algebra

Some fragmented notes salvaged from an old blog post.

I'm part of the way to understanding the [fundamental theorem of linear algebra](http://en.wikipedia.org/wiki/Fundamental_theorem_of_linear_algebra). I spent a long time pinning down what matrix multiplication means. I concluded there are at least three perspectives here: 1) the ij-th component of AB is (row i of A) dot (row j of B), which you might call the computational perspective (how to compute matrix multiplication). 2) Each row of AB is a linear combination of the rows of B, where the coefficients are the elements of the corresponding row of A (this is useful for elimination matrices). 3) Each column of AB is a linear combination of the columns of A, where the coefficients are the elements of the corresponding column of B. There are other perspectives, of course, like the crucial idea that matrices are linear transformations and matrix multiplication is composition of linear maps. But in terms of the row/column vectors, these 3 seem to sum it up.

I also read a short [proof](http://mathdl.maa.org/images/cms_upload/269057630842.pdf) (Mackiw 1995, "A Note on the Equality of the Column and Row Rank of a Matrix") of the equivalence of row ranks and column ranks, which was easily extended to a proof of the rank-nullity theorem. It presupposes an inner-product space, but it's quite the simple proof. I am not that far in the Strang book, but I think it will work out. Strang is more for computational practice and intuition, and I am skimming through it liberally (I have not bothered to do practice exercises so far).

There are a couple cool things about the equivalence of row rank and column rank. One is that you can forget about "row rank" and "column rank" and just say "rank". The second is that it shows that only square matrices can be invertible. Wait wuhhhh? Yeeeh bruh, the rank of a linear map is the dimension of the vector space that constitutes the image of the map. If we have a non-square mxn matrix, there are 2 possibilites: m < n, or m > n. if m < n, then the largest possible rank is m, so we're mapping a larger dimensional space into a smaller one. In other words, our linear map is not injective (different vectors get mapped into the same image vector), which means we can't find a left inverse. A right-inverse might be doable, but we can't get the two-sided inverse needed to make our matrix invertible. If m > n, we get the opposite problem: the rank is at most n, so our function maps n-dim vectors to an at-most n-dimensional subspace of the larger m-dimensional space. I.e., the function isn't surjective. A left-inverse might be doable, but there's no way we can arrange a right-inverse. So only square matrices can be invertible, though non-square matrices might still have one-sided inverses.

